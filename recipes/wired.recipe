
__license__   = 'GPL v3'
__copyright__ = '2010-2013, Darko Miletic <darko.miletic at gmail.com>'
'''
www.wired.com
'''

import re
from calibre import strftime
from calibre.web.feeds.news import BasicNewsRecipe

class Wired(BasicNewsRecipe):
    title                 = 'Wired Magazine'
    __author__            = 'Darko Miletic'
    description           = 'Gaming news'
    publisher             = 'Conde Nast Digital'
    category              = 'news, games, IT, gadgets'
    oldest_article        = 32
    max_articles_per_feed = 100
    no_stylesheets        = True
    encoding              = 'utf-8'
    use_embedded_content  = False
    masthead_url          = 'http://www.wired.com/images/home/wired_logo.gif'
    language              = 'en'
    publication_type      = 'magazine'
    extra_css             = """ 
                               h1, .entry-header{font-family: brandon-grotesque,anchor-web,Helvetica,Arial,sans-serif} 
                               .entry-header{display: block;}
                               .entry-header ul{ list-style-type:disc;}
                               .author, .entryDate, .entryTime, .entryEdit, .entryCategories{display: inline}
                               .entry-header li{text-transform: uppercase;}
                               div#container{font-family: 'Exchange SSm 4r', Georgia, serif} 
                            """
    index                 = 'http://www.wired.com/magazine/'

    preprocess_regexps = [(re.compile(r'<meta name="Title".*<title>', re.DOTALL|re.IGNORECASE),lambda match: '<title>')]
    conversion_options = {
                          'comment'   : description
                        , 'tags'      : category
                        , 'publisher' : publisher
                        , 'language'  : language
                        }

    keep_only_tags = [dict(name='div', attrs={'class':'post'})]
    remove_tags_after = dict(name='div', attrs={'id':'container'})
    remove_tags = [
                     dict(name=['object','embed','iframe','link','meta','base'])
                    ,dict(name='div', attrs={'class':['social-top','podcast_storyboard','tweetmeme_button']})
                    ,dict(attrs={'id':'ff_bottom_nav'})
                    ,dict(name='a',attrs={'href':'http://www.wired.com/app'})
                    ,dict(name='div', attrs={'id':'mag-bug'})
                  ]
    remove_attributes = ['height','width','lang','border','clear']


    def parse_index(self):
        totalfeeds = []
        soup   = self.index_to_soup(self.index)
        majorf = soup.find('div',attrs={'class':'entry'})
        if majorf:
           articles = []
           checker = []
           for a in majorf.findAll('a', href=True):
               if a['href'].startswith('http://www.wired.com/') and a['href'].endswith('/'):
                  title = self.tag_to_string(a)
                  url = a['href']
                  if title.lower() != 'read more' and url not in checker:
                      checker.append(url) 
                      articles.append({
                                          'title'      :title
                                         ,'date'       :strftime(self.timefmt)
                                         ,'url'        :a['href']
                                         ,'description':''
                                        })
           totalfeeds.append(('Articles', articles))
        return totalfeeds

    def get_cover_url(self):
        cover_url = None
        soup = self.index_to_soup(self.index)
        cover_item = soup.find('div',attrs={'class':'spread-image'})
        if cover_item:
           cover_url = 'http://www.wired.com' + cover_item.a.img['src']
        return cover_url

    def print_version(self, url):
        return url.rstrip('/') + '/all/1'

    def preprocess_html(self, soup):
        for item in soup.findAll(style=True):
            del item['style']
        for item in soup.findAll('a'):
            if item.string is not None:
               tstr = item.string
               item.replaceWith(tstr)
            else:
               item.name='span'
               for atrs in ['href','target','alt','title','name','id']:
                   if item.has_key(atrs):
                      del item[atrs]
        for item in soup.findAll('img'):
            if not item.has_key('alt'):
               item['alt'] = 'image'            
            if item.has_key('data-lazy-src'):
                item['src'] = item['data-lazy-src']
                del item['data-lazy-src']
        return soup
